{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9NAb9qosm15"
      },
      "outputs": [],
      "source": [
        "# Import library\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
        "\n",
        "# if we download model  manually\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"./models/gpt-j-6B\")"
      ],
      "metadata": {
        "id": "uVTDCAmUSjHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, my name is Mahdi\"\n",
        "\n",
        "tokens = tokenizer(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-jCIZdPs2_H",
        "outputId": "ef75a21a-4b23-440f-f055-3f72ac86201e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [15496, 11, 616, 1438, 318, 8882, 10989], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = tokenizer.encode(text)\n",
        "print(token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUol1CpvwPw2",
        "outputId": "64e7923a-3a37-4a49-d2e3-c02253784527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 616, 1438, 318, 8882, 10989]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_text = tokenizer.decode(token_ids)\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqUpwVW7xHNu",
        "outputId": "5a6ff095-484e-4360-ee1a-ed98aa60dd63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, my name is Mahdi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "rClpOvi4xObG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d87478de-13ad-45d0-c2aa-0a0eeaf84f9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'Ġmy', 'Ġname', 'Ġis', 'ĠMah', 'di']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "print(inputs)\n",
        "print(inputs.input_ids)\n",
        "print(inputs.input_ids.shape)\n",
        "print(tokenizer.decode(inputs.input_ids[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYyTBm5rUs-S",
        "outputId": "63f55911-7af8-482c-f3dd-abeeb848a9ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[15496,    11,   616,  1438,   318,  8882, 10989]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
            "tensor([[15496,    11,   616,  1438,   318,  8882, 10989]])\n",
            "torch.Size([1, 7])\n",
            "Hello, my name is Mahdi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.vocab[\"Mah\"], tokenizer.vocab[\"di\"])\n",
        "print(len(tokenizer.vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOvwejsHcUCT",
        "outputId": "7a8ca926-de75-4209-cba5-b9c20896e039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40936 10989\n",
            "50400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.special_tokens_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVVVwT6eiKos",
        "outputId": "618d7922-285c-437a-f3cf-0d3e8c41bd68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a padding token to the tokenizer's vocabulary to ensure consistent padding behavior\n",
        "tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
        "\n",
        "inputs = tokenizer(\n",
        "    [\"hello\", \"this is a longer sentence\"],  # Input texts to tokenize\n",
        "    padding=True,           # Enable padding to uniform length\n",
        "    truncation=True,        # truncates sequences longer than max_length\n",
        "    max_length=10,          # sets the maximum sequence length to 10 tokens\n",
        "    return_tensors=\"pt\"     # returns PyTorch tensors instead of Python lists\n",
        ")"
      ],
      "metadata": {
        "id": "05oLYKb6VZFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.special_tokens_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzRKH2h4i27j",
        "outputId": "acef93de-6e6d-4a29-bc35-3d352e8afb79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bos_token': '<|endoftext|>',\n",
              " 'eos_token': '<|endoftext|>',\n",
              " 'unk_token': '<|endoftext|>',\n",
              " 'pad_token': '<|pad|>'}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs.input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0TyyP--V7FL",
        "outputId": "2fb5622e-cfba-4314-9300-1b0fa20ae0e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[31373, 50400, 50400, 50400, 50400],\n",
            "        [ 5661,   318,   257,  2392,  6827]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokenizer.vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3hOc2MxW26K",
        "outputId": "549ed49c-1b8b-468d-c556-9e584160088d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id_to_word = {v: k for k, v in tokenizer.vocab.items()}\n",
        "print(id_to_word[40936], id_to_word[10989])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tnp03EZTXPvL",
        "outputId": "9c2f3eb0-8142-4602-e929-c0bf028682bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mah di\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(id_to_word[50400])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nzek2L5oX0Hz",
        "outputId": "c939a75e-7f31-4b51-abff-e67ebdbfd036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|pad|>\n"
          ]
        }
      ]
    }
  ]
}